import numpy as np
import pandas as pd
import tensorflow as tf
import keras.models 
import keras.layers.convolutional
#from  keras.optimizers import adam_v2
from keras import optimizers
from keras import optimizer_v2
from keras.models import Sequential
from keras.layers import *  # BatchNormalization, Activation,MaxPooling2D, Dropout, Flatten, Dense
import matplotlib.pyplot as plt

from keras.layers.convolutional import Conv2D
#importing tensorflow library and package
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau


PATH = 'C:\Projects\\archive\\images'
train_dir = PATH + '\\train'
validation_dir = PATH + '\\validation' 

BATCH_SIZE = 1                                              
IMG_SIZE = (48, 48) #(32,32) 
no_of_classes = 7
 
train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,
                                                            shuffle=True,
                                                            batch_size=BATCH_SIZE, 
                                                            image_size=IMG_SIZE)

#print(train_dataset[0:5])
training_Count = tf.data.experimental.cardinality(train_dataset).numpy()

validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,
                                                                 shuffle=True,
                                                                 batch_size=BATCH_SIZE, 
                                                                 image_size=IMG_SIZE)
                                                            
class_names = train_dataset.class_names
validation_Count = tf.data.experimental.cardinality(validation_dataset).numpy()
print('Number of training batches: %d' % training_Count)
print('Number of validation batches: %d' % validation_Count)

model = Sequential()

#1st CNN layer
model.add(Conv2D(64,(3,3),padding = 'same',input_shape = (48,48,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.25))
#2nd CNN layer
model.add(Conv2D(128,(5,5),padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout (0.25))

#3rd CNN layer
model.add(Conv2D(512,(3,3),padding = 'same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout (0.25))

#4th CNN layer
model.add(Conv2D(512,(3,3), padding='same'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())

#Fully connected 1st layer
model.add(Dense(256))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))


# Fully connected layer 2nd layer
model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))

model.add(Dense(no_of_classes, activation='softmax'))

opt = keras.optimizer_v2.adam.Adam(lr = 0.0001)
# model.compile(loss='categorical_crossentropy', optimizer=opt)(lr = 0.0001)
model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

#tf.keras.utils.plot_model(
#    model,
#    to_file="model.png",
#    show_shapes=False,
#    show_dtype=False,
#    show_layer_names=True,
#    rankdir="TB",
#    expand_nested=False,
#    dpi=96,
#    layer_range=None,
#)

checkpoint = ModelCheckpoint("./model.h5", monitor='val_acc', verbose=1, save_best_only=True, mode='max')
#Stopping training when a monitored metric has stopped improving.
early_stopping = EarlyStopping(monitor='val_loss',
                          min_delta=0,
                          patience=3,
                          verbose=1,
                          restore_best_weights=True
                          )

reduce_learningrate = ReduceLROnPlateau(monitor='val_loss',
                              factor=0.2,
                              patience=3,
                              verbose=1,
                              min_delta=0.0001)

callbacks_list = [early_stopping,checkpoint,reduce_learningrate]

epochs = 50

model.compile(loss='categorical_crossentropy',
              optimizer = keras.optimizer_v2.adam.Adam(lr=0.001),
              metrics=['accuracy'])

#fitting model with 48 epoch
history = model.fit_generator(generator=train_dataset,
                                steps_per_epoch=training_Count//BATCH_SIZE,
                                epochs=epochs,
                                validation_data = validation_dataset,
                                validation_steps = validation_Count//BATCH_SIZE,
                                callbacks=callbacks_list
                                )


###Copied
# Train neural network model
#model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])
#hist=model.fit(x_train,y_train,batch_size=128,epochs=30,validation_data=(x_test,y_test),verbose=2)

# Evaluate neural network model accuracy
#res=model.evaluate(x_test,y_test,verbose=0)
#print("Accuracy",res[1]*100)



# Accuracy graph
#plt.plot(hist.history['accuracy'])
#plt.plot(hist.history['val_accuracy'])
#plt.title('Model accuracy')
#plt.ylabel('Accuracy')
#plt.xlabel('Epoch')
##plt.legend(['Train','Validation'], loc='best')
#plt.grid()
#plt.show()

# Loss function graph
#plt.plot(hist.history['loss'])
#plt.plot(hist.history['val_loss'])
#plt.title('Model loss')
#plt.ylabel('Loss')
#plt.xlabel('Epoch')
#plt.legend(['Train','Validation'],loc='best')
#plt.grid()
#plt.show()
#model.save("my_cnn.h5")

plt.style.use('dark_background')

plt.figure(figsize=(20,10))
plt.subplot(1, 2, 1)
plt.suptitle('Optimizer : Adam', fontsize=10)
plt.ylabel('Loss', fontsize=16)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend(loc='upper right')

plt.subplot(1, 2, 2)
plt.ylabel('Accuracy', fontsize=16)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend(loc='lower right')
plt.show()